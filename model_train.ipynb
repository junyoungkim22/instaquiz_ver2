{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from allennlp.modules.elmo import Elmo\n",
    "\n",
    "from encoder import EncoderRNN\n",
    "from decoder import LuongAttnDecoderRNN\n",
    "from movie_line_process import loadLines, loadConversations, extractSentencePairs\n",
    "from voc import loadPrepareData, trimRareWords, normalizeString, makeVoc\n",
    "from voc import MIN_COUNT, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH, PAD_token, SOS_token, EOS_token\n",
    "from prepare_data import indexesFromSentence, batch2TrainData\n",
    "from train import trainIters\n",
    "from model_config import model_name, attn_model, hidden_size\n",
    "from model_config import encoder_n_layers, decoder_n_layers, dropout, batch_size, embedding_size\n",
    "from model_config import device, loadFilename, checkpoint_iter\n",
    "from model_config import save_dir, corpus_name, use_glove, use_elmo\n",
    "from train_config import clip, learning_rate, decoder_learning_ratio, n_iteration\n",
    "from train_config import print_every, save_every, evaluate_every\n",
    "from evaluate import GreedySearchDecoder, evaluateInput, dev_evaluate\n",
    "\n",
    "from squad_loader import prepare_par_pairs, prepare_sent_pairs\n",
    "from squad_loader import prepare_ans_sent_pairs\n",
    "from squad_loader import process_file\n",
    "\n",
    "from glove_loader import make_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_name = \"squad\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "#printLines(os.path.join(corpus, \"train-v2.0.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_train_squad_qa.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "data = process_file(\"train-v2.0.json\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n\\n')\n",
    "    pairs = prepare_ans_sent_pairs(data)\n",
    "    for pair in pairs:\n",
    "        writer.writerow(pair)\n",
    "    \n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load/Assemble voc and pairs\n",
    "pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "voc = makeVoc(corpus_name)\n",
    "# Print some pairs to validate\n",
    "'''\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "print(pairs[-1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(voc.num_words)\n",
    "# Trim voc\n",
    "#pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "voc.trim(MIN_COUNT)\n",
    "#print(voc.index2word[14274])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "test_pairs = []\n",
    "for _ in range(small_batch_size):\n",
    "    test_pairs.append(random.choice(pairs))\n",
    "#test_pairs = random.choice(pairs) for _ in range(small_batch_size)\n",
    "batches = batch2TrainData(voc, test_pairs)\n",
    "input_variable, lengths, target_variable, mask, max_target_len, answer_mask = batches\n",
    "print(input_variable.size())\n",
    "'''\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"answer mask:\", answer_mask)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)\n",
    "print(input_variable.size())\n",
    "'''\n",
    "input_variable = input_variable.to(device)\n",
    "print(input_variable)\n",
    "embedding(input_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, embedding_size)\n",
    "if loadFilename:\n",
    "    print(\"Loading model...\")\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "else:\n",
    "    if use_glove:\n",
    "        embedding.weight.data = torch.Tensor(make_weights(300, \"data/glove/glove.6B.300d.txt\", voc))\n",
    "    elif use_elmo:\n",
    "        print(\"Make ELMO embeddings...\")\n",
    "        options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "        weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "        embedding = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        embedding_size = 1024\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(embedding_size, hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, embedding_size, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder, voc)\n",
    "\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prepare development set\n",
    "dev_data = process_file(\"dev-v2.0.json\")\n",
    "dev_pairs = prepare_ans_sent_pairs(dev_data)\n",
    "print(len(dev_pairs))\n",
    "dev_pairs = dev_pairs[:500]\n",
    "\n",
    "random.Random(512).shuffle(dev_pairs)\n",
    "\n",
    "\n",
    "#dev_evaluate(encoder, decoder, dev_pairs, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterationsjjjjjj\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, dev_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, evaluate_every, clip, corpus_name, loadFilename, searcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective GreedySearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"what river originally bounded the duchy\"\n",
    "text2 = \"what river\"\n",
    "\n",
    "input1 = text1.split(' ')\n",
    "input2 = text2.split(' ')\n",
    "\n",
    "print(sentence_bleu([input1], input2, weights=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(voc.index2word[10489])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
