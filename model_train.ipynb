{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from encoder import EncoderRNN\n",
    "from decoder import LuongAttnDecoderRNN\n",
    "from movie_line_process import loadLines, loadConversations, extractSentencePairs\n",
    "from voc import loadPrepareData, trimRareWords, normalizeString, makeVoc\n",
    "from voc import MIN_COUNT, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH, PAD_token, SOS_token, EOS_token\n",
    "from prepare_data import indexesFromSentence, batch2TrainData\n",
    "from train import trainIters\n",
    "from model_config import model_name, attn_model, hidden_size\n",
    "from model_config import encoder_n_layers, decoder_n_layers, dropout, batch_size\n",
    "from model_config import device, loadFilename, checkpoint_iter\n",
    "from train_config import clip, learning_rate, decoder_learning_ratio, n_iteration\n",
    "from train_config import print_every, save_every\n",
    "from evaluate import GreedySearchDecoder, evaluateInput\n",
    "\n",
    "from squad_loader import prepare_par_pairs, prepare_sent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"squad\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "#printLines(os.path.join(corpus, \"train-v2.0.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing newly formatted file...\n",
      "Processing SquAD dataset...\n",
      "Processing done!\n",
      "\n",
      "Sample lines from file:\n",
      "b\" Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame   in the late 1990s   as lead singer of R&B girl-group Destiny's Child\\tWhen did Beyonce start becoming popular?\\n\"\n",
      "b'\\n'\n",
      "b\" Born and raised in Houston, Texas, she performed in various   singing and dancing   competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child\\tWhat areas did Beyonce compete in when she was growing up?\\n\"\n",
      "b'\\n'\n",
      "b'\" Their hiatus saw the release of Beyonc\\xc3\\xa9\\'s debut album, Dangerously in Love (  2003  ), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"\"Crazy in Love\"\" and \"\"Baby Boy\"\"\"\\tWhen did Beyonce leave Destiny\\'s Child and become a solo singer?\\n'\n",
      "b'\\n'\n",
      "b\" Born and raised in   Houston, Texas  , she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child\\tIn what city and state did Beyonce  grow up? \\n\"\n",
      "b'\\n'\n",
      "b\" Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the   late 1990s   as lead singer of R&B girl-group Destiny's Child\\tIn which decade did Beyonce become famous?\\n\"\n",
      "b'\\n'\n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_dev_squad_qa.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n\\n')\n",
    "    pairs = prepare_sent_pairs()\n",
    "    for pair in pairs:\n",
    "        writer.writerow(pair)\n",
    "    \n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 86821 sentence pairs\n",
      "Trimmed to 86585 sentence pairs\n",
      "Counting words...\n",
      "Processing SquAD dataset...\n",
      "Processing done!\n",
      "\n",
      "pairs:\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'when did beyonce start becoming popular ?']\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'what areas did beyonce compete in when she was growing up ?']\n",
      "['their hiatus saw the release of beyonce s debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 number one singles crazy in love and baby boy', 'when did beyonce leave destiny s child and become a solo singer ?']\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'in what city and state did beyonce grow up ?']\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'in which decade did beyonce become famous ?']\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'in what r b group was she the lead singer ?']\n",
      "['their hiatus saw the release of beyonce s debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 number one singles crazy in love and baby boy', 'what album made her a worldwide known artist ?']\n",
      "['managed by her father mathew knowles the group became one of the world s best selling girl groups of all time', 'who managed the destiny s child group ?']\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'when did beyonce rise to fame ?']\n",
      "['born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child', 'what role did beyonce have in destiny s child ?']\n",
      "['kathmandu metropolitan city kmc in order to promote international relations has established an international relations secretariat irc', 'what is kmc an initialism of ?']\n"
     ]
    }
   ],
   "source": [
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "voc = makeVoc(corpus_name)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "print(pairs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 39927 / 82663 = 0.4830\n"
     ]
    }
   ],
   "source": [
    "#print(voc.num_words)\n",
    "# Trim voc\n",
    "#pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "voc.trim(MIN_COUNT)\n",
    "#print(voc.index2word[14274])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[24992, 35743, 24992, 16012,  7734],\n",
      "        [29237, 24992, 26097, 27437,  4391],\n",
      "        [25438,  6273, 38701, 21651, 10104],\n",
      "        [17392, 38701, 16387,  1761, 25812],\n",
      "        [ 1837, 28993, 25812,  7603, 30421],\n",
      "        [25812, 16012, 18736, 31437, 14547],\n",
      "        [ 5571, 19910, 17375, 34783, 28992],\n",
      "        [ 4862, 39047, 14902, 14793,  6958],\n",
      "        [30867, 24992, 16143, 28253,     3],\n",
      "        [24884, 24551, 24850,  1974,     2],\n",
      "        [ 1837,  8482, 11643, 26574,     0],\n",
      "        [38701, 25812, 37116, 19359,     0],\n",
      "        [36314, 24992, 25110, 17375,     0],\n",
      "        [24469,  5915,  4448, 30930,     0],\n",
      "        [31080, 17320, 33946, 20352,     0],\n",
      "        [24992, 38701, 36294, 19924,     0],\n",
      "        [  298, 24992, 24992, 24992,     0],\n",
      "        [38701, 27620, 28598, 29684,     0],\n",
      "        [ 8149, 18242, 39047, 17375,     0],\n",
      "        [33946,  1486, 32020,  6996,     0],\n",
      "        [35970, 35970, 17375, 21651,     0],\n",
      "        [24992, 17270, 24992,  1974,     0],\n",
      "        [33167,  1825, 35064, 19359,     0],\n",
      "        [25812, 38701, 24940,     2,     0],\n",
      "        [28716, 27620,     2,     0,     0],\n",
      "        [ 2006, 39047,     0,     0,     0],\n",
      "        [38701, 28993,     0,     0,     0],\n",
      "        [24992, 16167,     0,     0,     0],\n",
      "        [36893, 25998,     0,     0,     0],\n",
      "        [33946,     2,     0,     0,     0],\n",
      "        [ 6762,     0,     0,     0,     0],\n",
      "        [33946,     0,     0,     0,     0],\n",
      "        [28253,     0,     0,     0,     0],\n",
      "        [36450,     0,     0,     0,     0],\n",
      "        [38701,     0,     0,     0,     0],\n",
      "        [10751,     0,     0,     0,     0],\n",
      "        [31167,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "lengths: tensor([38, 30, 25, 24, 10])\n",
      "target_variable: tensor([[25922,  2886, 25922, 16284, 25922],\n",
      "        [ 7563,  9374,  5647, 19840, 12191],\n",
      "        [24992,  7563, 38701, 24992, 38701],\n",
      "        [ 4862, 24992, 26097, 28598, 30421],\n",
      "        [ 1837,  6273,  7563, 22544, 14547],\n",
      "        [38701, 38701, 16387,  8364, 28992],\n",
      "        [36314, 28993, 25812, 22983,  7563],\n",
      "        [ 5571, 16033, 18736,  7106, 34581],\n",
      "        [32283,  2127, 24786, 23403, 35863],\n",
      "        [ 2127,     2,  2127, 26574,  6958],\n",
      "        [    2,     0,     2, 33946, 31885],\n",
      "        [    0,     0,     0, 38701,  2127],\n",
      "        [    0,     0,     0, 27437,     2],\n",
      "        [    0,     0,     0,  2127,     0],\n",
      "        [    0,     0,     0,     2,     0]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0]], dtype=torch.uint8)\n",
      "max_target_len: 15\n"
     ]
    }
   ],
   "source": [
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 10; Percent complete: 0.3%; Average loss: 2.5526\n",
      "Iteration: 20; Percent complete: 0.7%; Average loss: 2.5791\n",
      "Iteration: 30; Percent complete: 1.0%; Average loss: 2.5328\n",
      "Iteration: 40; Percent complete: 1.3%; Average loss: 2.4961\n",
      "Iteration: 50; Percent complete: 1.7%; Average loss: 2.6192\n",
      "Iteration: 60; Percent complete: 2.0%; Average loss: 2.5750\n",
      "Iteration: 70; Percent complete: 2.3%; Average loss: 2.5523\n",
      "Iteration: 80; Percent complete: 2.7%; Average loss: 2.6706\n",
      "Iteration: 90; Percent complete: 3.0%; Average loss: 2.6813\n",
      "Iteration: 100; Percent complete: 3.3%; Average loss: 2.6381\n",
      "Iteration: 110; Percent complete: 3.7%; Average loss: 2.5897\n",
      "Iteration: 120; Percent complete: 4.0%; Average loss: 2.5830\n",
      "Iteration: 130; Percent complete: 4.3%; Average loss: 2.7236\n",
      "Iteration: 140; Percent complete: 4.7%; Average loss: 2.6574\n",
      "Iteration: 150; Percent complete: 5.0%; Average loss: 2.7179\n",
      "Iteration: 160; Percent complete: 5.3%; Average loss: 2.6085\n",
      "Iteration: 170; Percent complete: 5.7%; Average loss: 2.6314\n",
      "Iteration: 180; Percent complete: 6.0%; Average loss: 2.6209\n",
      "Iteration: 190; Percent complete: 6.3%; Average loss: 2.6104\n",
      "Iteration: 200; Percent complete: 6.7%; Average loss: 2.6671\n",
      "Iteration: 210; Percent complete: 7.0%; Average loss: 2.6645\n",
      "Iteration: 220; Percent complete: 7.3%; Average loss: 2.7068\n",
      "Iteration: 230; Percent complete: 7.7%; Average loss: 2.6879\n",
      "Iteration: 240; Percent complete: 8.0%; Average loss: 2.7590\n",
      "Iteration: 250; Percent complete: 8.3%; Average loss: 2.6917\n",
      "Iteration: 260; Percent complete: 8.7%; Average loss: 2.6746\n",
      "Iteration: 270; Percent complete: 9.0%; Average loss: 2.7554\n",
      "Iteration: 280; Percent complete: 9.3%; Average loss: 2.6750\n",
      "Iteration: 290; Percent complete: 9.7%; Average loss: 2.6189\n",
      "Iteration: 300; Percent complete: 10.0%; Average loss: 2.6645\n",
      "Iteration: 310; Percent complete: 10.3%; Average loss: 2.6669\n",
      "Iteration: 320; Percent complete: 10.7%; Average loss: 2.5580\n",
      "Iteration: 330; Percent complete: 11.0%; Average loss: 2.6931\n",
      "Iteration: 340; Percent complete: 11.3%; Average loss: 2.6177\n",
      "Iteration: 350; Percent complete: 11.7%; Average loss: 2.7095\n",
      "Iteration: 360; Percent complete: 12.0%; Average loss: 2.6128\n",
      "Iteration: 370; Percent complete: 12.3%; Average loss: 2.6315\n",
      "Iteration: 380; Percent complete: 12.7%; Average loss: 2.6489\n",
      "Iteration: 390; Percent complete: 13.0%; Average loss: 2.6403\n",
      "Iteration: 400; Percent complete: 13.3%; Average loss: 2.6561\n",
      "Iteration: 410; Percent complete: 13.7%; Average loss: 2.6540\n",
      "Iteration: 420; Percent complete: 14.0%; Average loss: 2.7006\n",
      "Iteration: 430; Percent complete: 14.3%; Average loss: 2.6211\n",
      "Iteration: 440; Percent complete: 14.7%; Average loss: 2.6636\n",
      "Iteration: 450; Percent complete: 15.0%; Average loss: 2.6271\n",
      "Iteration: 460; Percent complete: 15.3%; Average loss: 2.5505\n",
      "Iteration: 470; Percent complete: 15.7%; Average loss: 2.6678\n",
      "Iteration: 480; Percent complete: 16.0%; Average loss: 2.6341\n",
      "Iteration: 490; Percent complete: 16.3%; Average loss: 2.7251\n",
      "Iteration: 500; Percent complete: 16.7%; Average loss: 2.6739\n",
      "Iteration: 510; Percent complete: 17.0%; Average loss: 2.6244\n",
      "Iteration: 520; Percent complete: 17.3%; Average loss: 2.6890\n",
      "Iteration: 530; Percent complete: 17.7%; Average loss: 2.6325\n",
      "Iteration: 540; Percent complete: 18.0%; Average loss: 2.6727\n",
      "Iteration: 550; Percent complete: 18.3%; Average loss: 2.6311\n",
      "Iteration: 560; Percent complete: 18.7%; Average loss: 2.6678\n",
      "Iteration: 570; Percent complete: 19.0%; Average loss: 2.6263\n",
      "Iteration: 580; Percent complete: 19.3%; Average loss: 2.6974\n",
      "Iteration: 590; Percent complete: 19.7%; Average loss: 2.6308\n",
      "Iteration: 600; Percent complete: 20.0%; Average loss: 2.6190\n",
      "Iteration: 610; Percent complete: 20.3%; Average loss: 2.6649\n",
      "Iteration: 620; Percent complete: 20.7%; Average loss: 2.6021\n",
      "Iteration: 630; Percent complete: 21.0%; Average loss: 2.5944\n",
      "Iteration: 640; Percent complete: 21.3%; Average loss: 2.6312\n",
      "Iteration: 650; Percent complete: 21.7%; Average loss: 2.6023\n",
      "Iteration: 660; Percent complete: 22.0%; Average loss: 2.5651\n",
      "Iteration: 670; Percent complete: 22.3%; Average loss: 2.6007\n",
      "Iteration: 680; Percent complete: 22.7%; Average loss: 2.5140\n",
      "Iteration: 690; Percent complete: 23.0%; Average loss: 2.5901\n",
      "Iteration: 700; Percent complete: 23.3%; Average loss: 2.5995\n",
      "Iteration: 710; Percent complete: 23.7%; Average loss: 2.5722\n",
      "Iteration: 720; Percent complete: 24.0%; Average loss: 2.6238\n",
      "Iteration: 730; Percent complete: 24.3%; Average loss: 2.6506\n",
      "Iteration: 740; Percent complete: 24.7%; Average loss: 2.6322\n",
      "Iteration: 750; Percent complete: 25.0%; Average loss: 2.6165\n",
      "Iteration: 760; Percent complete: 25.3%; Average loss: 2.6383\n",
      "Iteration: 770; Percent complete: 25.7%; Average loss: 2.5583\n",
      "Iteration: 780; Percent complete: 26.0%; Average loss: 2.6042\n",
      "Iteration: 790; Percent complete: 26.3%; Average loss: 2.6270\n",
      "Iteration: 800; Percent complete: 26.7%; Average loss: 2.5280\n",
      "Iteration: 810; Percent complete: 27.0%; Average loss: 2.5835\n",
      "Iteration: 820; Percent complete: 27.3%; Average loss: 2.6247\n",
      "Iteration: 830; Percent complete: 27.7%; Average loss: 2.5538\n",
      "Iteration: 840; Percent complete: 28.0%; Average loss: 2.5093\n",
      "Iteration: 850; Percent complete: 28.3%; Average loss: 2.5250\n",
      "Iteration: 860; Percent complete: 28.7%; Average loss: 2.5069\n",
      "Iteration: 870; Percent complete: 29.0%; Average loss: 2.5543\n",
      "Iteration: 880; Percent complete: 29.3%; Average loss: 2.5918\n",
      "Iteration: 890; Percent complete: 29.7%; Average loss: 2.4949\n",
      "Iteration: 900; Percent complete: 30.0%; Average loss: 2.5929\n",
      "Iteration: 910; Percent complete: 30.3%; Average loss: 2.5330\n",
      "Iteration: 920; Percent complete: 30.7%; Average loss: 2.5627\n",
      "Iteration: 930; Percent complete: 31.0%; Average loss: 2.5529\n",
      "Iteration: 940; Percent complete: 31.3%; Average loss: 2.6114\n",
      "Iteration: 950; Percent complete: 31.7%; Average loss: 2.5769\n",
      "Iteration: 960; Percent complete: 32.0%; Average loss: 2.5539\n",
      "Iteration: 970; Percent complete: 32.3%; Average loss: 2.5266\n",
      "Iteration: 980; Percent complete: 32.7%; Average loss: 2.4589\n",
      "Iteration: 990; Percent complete: 33.0%; Average loss: 2.5019\n",
      "Iteration: 1000; Percent complete: 33.3%; Average loss: 2.4536\n",
      "Iteration: 1010; Percent complete: 33.7%; Average loss: 2.4864\n",
      "Iteration: 1020; Percent complete: 34.0%; Average loss: 2.4686\n",
      "Iteration: 1030; Percent complete: 34.3%; Average loss: 2.5699\n",
      "Iteration: 1040; Percent complete: 34.7%; Average loss: 2.5237\n",
      "Iteration: 1050; Percent complete: 35.0%; Average loss: 2.5172\n",
      "Iteration: 1060; Percent complete: 35.3%; Average loss: 2.5829\n",
      "Iteration: 1070; Percent complete: 35.7%; Average loss: 2.5359\n",
      "Iteration: 1080; Percent complete: 36.0%; Average loss: 2.4978\n",
      "Iteration: 1090; Percent complete: 36.3%; Average loss: 2.5132\n",
      "Iteration: 1100; Percent complete: 36.7%; Average loss: 2.4881\n",
      "Iteration: 1110; Percent complete: 37.0%; Average loss: 2.5009\n",
      "Iteration: 1120; Percent complete: 37.3%; Average loss: 2.6286\n",
      "Iteration: 1130; Percent complete: 37.7%; Average loss: 2.4979\n",
      "Iteration: 1140; Percent complete: 38.0%; Average loss: 2.5528\n",
      "Iteration: 1150; Percent complete: 38.3%; Average loss: 2.5179\n",
      "Iteration: 1160; Percent complete: 38.7%; Average loss: 2.5144\n",
      "Iteration: 1170; Percent complete: 39.0%; Average loss: 2.4111\n",
      "Iteration: 1180; Percent complete: 39.3%; Average loss: 2.5306\n",
      "Iteration: 1190; Percent complete: 39.7%; Average loss: 2.6036\n",
      "Iteration: 1200; Percent complete: 40.0%; Average loss: 2.5018\n",
      "Iteration: 1210; Percent complete: 40.3%; Average loss: 2.5258\n",
      "Iteration: 1220; Percent complete: 40.7%; Average loss: 2.4708\n",
      "Iteration: 1230; Percent complete: 41.0%; Average loss: 2.5238\n",
      "Iteration: 1240; Percent complete: 41.3%; Average loss: 2.4709\n",
      "Iteration: 1250; Percent complete: 41.7%; Average loss: 2.5046\n",
      "Iteration: 1260; Percent complete: 42.0%; Average loss: 2.5503\n",
      "Iteration: 1270; Percent complete: 42.3%; Average loss: 2.5681\n",
      "Iteration: 1280; Percent complete: 42.7%; Average loss: 2.4678\n",
      "Iteration: 1290; Percent complete: 43.0%; Average loss: 2.5712\n",
      "Iteration: 1300; Percent complete: 43.3%; Average loss: 2.4588\n",
      "Iteration: 1310; Percent complete: 43.7%; Average loss: 2.4432\n",
      "Iteration: 1320; Percent complete: 44.0%; Average loss: 2.4441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1330; Percent complete: 44.3%; Average loss: 2.4355\n",
      "Iteration: 1340; Percent complete: 44.7%; Average loss: 2.4636\n",
      "Iteration: 1350; Percent complete: 45.0%; Average loss: 2.4563\n",
      "Iteration: 1360; Percent complete: 45.3%; Average loss: 2.4075\n",
      "Iteration: 1370; Percent complete: 45.7%; Average loss: 2.4780\n",
      "Iteration: 1380; Percent complete: 46.0%; Average loss: 2.4145\n",
      "Iteration: 1390; Percent complete: 46.3%; Average loss: 2.5272\n",
      "Iteration: 1400; Percent complete: 46.7%; Average loss: 2.4811\n",
      "Iteration: 1410; Percent complete: 47.0%; Average loss: 2.4501\n",
      "Iteration: 1420; Percent complete: 47.3%; Average loss: 2.4198\n",
      "Iteration: 1430; Percent complete: 47.7%; Average loss: 2.4869\n",
      "Iteration: 1440; Percent complete: 48.0%; Average loss: 2.4668\n",
      "Iteration: 1450; Percent complete: 48.3%; Average loss: 2.3614\n",
      "Iteration: 1460; Percent complete: 48.7%; Average loss: 2.4502\n",
      "Iteration: 1470; Percent complete: 49.0%; Average loss: 2.4074\n",
      "Iteration: 1480; Percent complete: 49.3%; Average loss: 2.4384\n",
      "Iteration: 1490; Percent complete: 49.7%; Average loss: 2.4431\n",
      "Iteration: 1500; Percent complete: 50.0%; Average loss: 2.4649\n",
      "Iteration: 1510; Percent complete: 50.3%; Average loss: 2.3976\n",
      "Iteration: 1520; Percent complete: 50.7%; Average loss: 2.3920\n",
      "Iteration: 1530; Percent complete: 51.0%; Average loss: 2.4420\n",
      "Iteration: 1540; Percent complete: 51.3%; Average loss: 2.4633\n",
      "Iteration: 1550; Percent complete: 51.7%; Average loss: 2.3313\n",
      "Iteration: 1560; Percent complete: 52.0%; Average loss: 2.4252\n",
      "Iteration: 1570; Percent complete: 52.3%; Average loss: 2.4052\n",
      "Iteration: 1580; Percent complete: 52.7%; Average loss: 2.4334\n",
      "Iteration: 1590; Percent complete: 53.0%; Average loss: 2.3900\n",
      "Iteration: 1600; Percent complete: 53.3%; Average loss: 2.4339\n",
      "Iteration: 1610; Percent complete: 53.7%; Average loss: 2.3954\n",
      "Iteration: 1620; Percent complete: 54.0%; Average loss: 2.4240\n",
      "Iteration: 1630; Percent complete: 54.3%; Average loss: 2.4158\n",
      "Iteration: 1640; Percent complete: 54.7%; Average loss: 2.4621\n",
      "Iteration: 1650; Percent complete: 55.0%; Average loss: 2.4317\n",
      "Iteration: 1660; Percent complete: 55.3%; Average loss: 2.3231\n",
      "Iteration: 1670; Percent complete: 55.7%; Average loss: 2.3775\n",
      "Iteration: 1680; Percent complete: 56.0%; Average loss: 2.5020\n",
      "Iteration: 1690; Percent complete: 56.3%; Average loss: 2.4671\n",
      "Iteration: 1700; Percent complete: 56.7%; Average loss: 2.3178\n",
      "Iteration: 1710; Percent complete: 57.0%; Average loss: 2.4325\n",
      "Iteration: 1720; Percent complete: 57.3%; Average loss: 2.3662\n",
      "Iteration: 1730; Percent complete: 57.7%; Average loss: 2.3557\n",
      "Iteration: 1740; Percent complete: 58.0%; Average loss: 2.3247\n",
      "Iteration: 1750; Percent complete: 58.3%; Average loss: 2.3597\n",
      "Iteration: 1760; Percent complete: 58.7%; Average loss: 2.4097\n",
      "Iteration: 1770; Percent complete: 59.0%; Average loss: 2.3757\n",
      "Iteration: 1780; Percent complete: 59.3%; Average loss: 2.3862\n",
      "Iteration: 1790; Percent complete: 59.7%; Average loss: 2.3960\n",
      "Iteration: 1800; Percent complete: 60.0%; Average loss: 2.3692\n",
      "Iteration: 1810; Percent complete: 60.3%; Average loss: 2.3294\n",
      "Iteration: 1820; Percent complete: 60.7%; Average loss: 2.3959\n",
      "Iteration: 1830; Percent complete: 61.0%; Average loss: 2.3646\n",
      "Iteration: 1840; Percent complete: 61.3%; Average loss: 2.2809\n",
      "Iteration: 1850; Percent complete: 61.7%; Average loss: 2.3551\n",
      "Iteration: 1860; Percent complete: 62.0%; Average loss: 2.3728\n",
      "Iteration: 1870; Percent complete: 62.3%; Average loss: 2.2977\n",
      "Iteration: 1880; Percent complete: 62.7%; Average loss: 2.3173\n",
      "Iteration: 1890; Percent complete: 63.0%; Average loss: 2.3640\n",
      "Iteration: 1900; Percent complete: 63.3%; Average loss: 2.3724\n",
      "Iteration: 1910; Percent complete: 63.7%; Average loss: 2.3032\n",
      "Iteration: 1920; Percent complete: 64.0%; Average loss: 2.3562\n",
      "Iteration: 1930; Percent complete: 64.3%; Average loss: 2.3079\n",
      "Iteration: 1940; Percent complete: 64.7%; Average loss: 2.3019\n",
      "Iteration: 1950; Percent complete: 65.0%; Average loss: 2.4558\n",
      "Iteration: 1960; Percent complete: 65.3%; Average loss: 2.3952\n",
      "Iteration: 1970; Percent complete: 65.7%; Average loss: 2.4060\n",
      "Iteration: 1980; Percent complete: 66.0%; Average loss: 2.4492\n",
      "Iteration: 1990; Percent complete: 66.3%; Average loss: 2.3678\n",
      "Iteration: 2000; Percent complete: 66.7%; Average loss: 2.3822\n",
      "Iteration: 2010; Percent complete: 67.0%; Average loss: 2.4183\n",
      "Iteration: 2020; Percent complete: 67.3%; Average loss: 2.3924\n",
      "Iteration: 2030; Percent complete: 67.7%; Average loss: 2.3604\n",
      "Iteration: 2040; Percent complete: 68.0%; Average loss: 2.3583\n",
      "Iteration: 2050; Percent complete: 68.3%; Average loss: 2.3230\n",
      "Iteration: 2060; Percent complete: 68.7%; Average loss: 2.3990\n",
      "Iteration: 2070; Percent complete: 69.0%; Average loss: 2.2982\n",
      "Iteration: 2080; Percent complete: 69.3%; Average loss: 2.4452\n",
      "Iteration: 2090; Percent complete: 69.7%; Average loss: 2.2994\n",
      "Iteration: 2100; Percent complete: 70.0%; Average loss: 2.3859\n",
      "Iteration: 2110; Percent complete: 70.3%; Average loss: 2.3711\n",
      "Iteration: 2120; Percent complete: 70.7%; Average loss: 2.2549\n",
      "Iteration: 2130; Percent complete: 71.0%; Average loss: 2.3231\n",
      "Iteration: 2140; Percent complete: 71.3%; Average loss: 2.3737\n",
      "Iteration: 2150; Percent complete: 71.7%; Average loss: 2.3586\n",
      "Iteration: 2160; Percent complete: 72.0%; Average loss: 2.3095\n",
      "Iteration: 2170; Percent complete: 72.3%; Average loss: 2.3165\n",
      "Iteration: 2180; Percent complete: 72.7%; Average loss: 2.3193\n",
      "Iteration: 2190; Percent complete: 73.0%; Average loss: 2.3462\n",
      "Iteration: 2200; Percent complete: 73.3%; Average loss: 2.3346\n",
      "Iteration: 2210; Percent complete: 73.7%; Average loss: 2.3189\n",
      "Iteration: 2220; Percent complete: 74.0%; Average loss: 2.2941\n",
      "Iteration: 2230; Percent complete: 74.3%; Average loss: 2.3241\n",
      "Iteration: 2240; Percent complete: 74.7%; Average loss: 2.3505\n",
      "Iteration: 2250; Percent complete: 75.0%; Average loss: 2.3311\n",
      "Iteration: 2260; Percent complete: 75.3%; Average loss: 2.2940\n",
      "Iteration: 2270; Percent complete: 75.7%; Average loss: 2.2567\n",
      "Iteration: 2280; Percent complete: 76.0%; Average loss: 2.2786\n",
      "Iteration: 2290; Percent complete: 76.3%; Average loss: 2.3261\n",
      "Iteration: 2300; Percent complete: 76.7%; Average loss: 2.3391\n",
      "Iteration: 2310; Percent complete: 77.0%; Average loss: 2.2818\n",
      "Iteration: 2320; Percent complete: 77.3%; Average loss: 2.3436\n",
      "Iteration: 2330; Percent complete: 77.7%; Average loss: 2.3473\n",
      "Iteration: 2340; Percent complete: 78.0%; Average loss: 2.2446\n",
      "Iteration: 2350; Percent complete: 78.3%; Average loss: 2.2669\n",
      "Iteration: 2360; Percent complete: 78.7%; Average loss: 2.3152\n",
      "Iteration: 2370; Percent complete: 79.0%; Average loss: 2.2587\n",
      "Iteration: 2380; Percent complete: 79.3%; Average loss: 2.2993\n",
      "Iteration: 2390; Percent complete: 79.7%; Average loss: 2.2938\n",
      "Iteration: 2400; Percent complete: 80.0%; Average loss: 2.2853\n",
      "Iteration: 2410; Percent complete: 80.3%; Average loss: 2.3026\n",
      "Iteration: 2420; Percent complete: 80.7%; Average loss: 2.2591\n",
      "Iteration: 2430; Percent complete: 81.0%; Average loss: 2.2893\n",
      "Iteration: 2440; Percent complete: 81.3%; Average loss: 2.1833\n",
      "Iteration: 2450; Percent complete: 81.7%; Average loss: 2.3317\n",
      "Iteration: 2460; Percent complete: 82.0%; Average loss: 2.3111\n",
      "Iteration: 2470; Percent complete: 82.3%; Average loss: 2.3343\n",
      "Iteration: 2480; Percent complete: 82.7%; Average loss: 2.2823\n",
      "Iteration: 2490; Percent complete: 83.0%; Average loss: 2.2923\n",
      "Iteration: 2500; Percent complete: 83.3%; Average loss: 2.2293\n",
      "Iteration: 2510; Percent complete: 83.7%; Average loss: 2.2755\n",
      "Iteration: 2520; Percent complete: 84.0%; Average loss: 2.3407\n",
      "Iteration: 2530; Percent complete: 84.3%; Average loss: 2.2508\n",
      "Iteration: 2540; Percent complete: 84.7%; Average loss: 2.2967\n",
      "Iteration: 2550; Percent complete: 85.0%; Average loss: 2.3395\n",
      "Iteration: 2560; Percent complete: 85.3%; Average loss: 2.2281\n",
      "Iteration: 2570; Percent complete: 85.7%; Average loss: 2.2490\n",
      "Iteration: 2580; Percent complete: 86.0%; Average loss: 2.2786\n",
      "Iteration: 2590; Percent complete: 86.3%; Average loss: 2.3360\n",
      "Iteration: 2600; Percent complete: 86.7%; Average loss: 2.2272\n",
      "Iteration: 2610; Percent complete: 87.0%; Average loss: 2.2237\n",
      "Iteration: 2620; Percent complete: 87.3%; Average loss: 2.2669\n",
      "Iteration: 2630; Percent complete: 87.7%; Average loss: 2.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2640; Percent complete: 88.0%; Average loss: 2.2506\n",
      "Iteration: 2650; Percent complete: 88.3%; Average loss: 2.2089\n",
      "Iteration: 2660; Percent complete: 88.7%; Average loss: 2.2857\n",
      "Iteration: 2670; Percent complete: 89.0%; Average loss: 2.1899\n",
      "Iteration: 2680; Percent complete: 89.3%; Average loss: 2.2212\n",
      "Iteration: 2690; Percent complete: 89.7%; Average loss: 2.2137\n",
      "Iteration: 2700; Percent complete: 90.0%; Average loss: 2.2282\n",
      "Iteration: 2710; Percent complete: 90.3%; Average loss: 2.1780\n",
      "Iteration: 2720; Percent complete: 90.7%; Average loss: 2.2228\n",
      "Iteration: 2730; Percent complete: 91.0%; Average loss: 2.3413\n",
      "Iteration: 2740; Percent complete: 91.3%; Average loss: 2.2295\n",
      "Iteration: 2750; Percent complete: 91.7%; Average loss: 2.2032\n",
      "Iteration: 2760; Percent complete: 92.0%; Average loss: 2.3101\n",
      "Iteration: 2770; Percent complete: 92.3%; Average loss: 2.1837\n",
      "Iteration: 2780; Percent complete: 92.7%; Average loss: 2.2665\n",
      "Iteration: 2790; Percent complete: 93.0%; Average loss: 2.2568\n",
      "Iteration: 2800; Percent complete: 93.3%; Average loss: 2.1925\n",
      "Iteration: 2810; Percent complete: 93.7%; Average loss: 2.2088\n",
      "Iteration: 2820; Percent complete: 94.0%; Average loss: 2.2055\n",
      "Iteration: 2830; Percent complete: 94.3%; Average loss: 2.2502\n",
      "Iteration: 2840; Percent complete: 94.7%; Average loss: 2.3058\n",
      "Iteration: 2850; Percent complete: 95.0%; Average loss: 2.2226\n",
      "Iteration: 2860; Percent complete: 95.3%; Average loss: 2.2394\n",
      "Iteration: 2870; Percent complete: 95.7%; Average loss: 2.2178\n",
      "Iteration: 2880; Percent complete: 96.0%; Average loss: 2.1451\n",
      "Iteration: 2890; Percent complete: 96.3%; Average loss: 2.2045\n",
      "Iteration: 2900; Percent complete: 96.7%; Average loss: 2.1688\n",
      "Iteration: 2910; Percent complete: 97.0%; Average loss: 2.2637\n",
      "Iteration: 2920; Percent complete: 97.3%; Average loss: 2.1972\n",
      "Iteration: 2930; Percent complete: 97.7%; Average loss: 2.2079\n",
      "Iteration: 2940; Percent complete: 98.0%; Average loss: 2.1975\n",
      "Iteration: 2950; Percent complete: 98.3%; Average loss: 2.2925\n",
      "Iteration: 2960; Percent complete: 98.7%; Average loss: 2.1628\n",
      "Iteration: 2970; Percent complete: 99.0%; Average loss: 2.2314\n",
      "Iteration: 2980; Percent complete: 99.3%; Average loss: 2.2530\n",
      "Iteration: 2990; Percent complete: 99.7%; Average loss: 2.2215\n",
      "Iteration: 3000; Percent complete: 100.0%; Average loss: 2.2134\n"
     ]
    }
   ],
   "source": [
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, 3000, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> According to professor Jeffrey Pfeffer from Stanford, there are only three levels in the organization and CEO Jim Goodnight has 27 people who directly report to him.\n",
      "Bot: how many members does the new york city have ?\n",
      "> Employees are encouraged to do volunteer work and the company makes donation to non-profits where employees are involved.\n",
      "Bot: what are the two companies that are connected to the company ? t receive a result of the revolution ?\n",
      "> SAS started building its current headquarters in a forested area of Cary, North Carolina in 1980.\n",
      "Bot: what is the name of the area in miami ? is what ?\n",
      "> Stanlow & Thornton railway station is located within the Stanlow Refinery in Cheshire, England.\n",
      "Bot: what is the railway station in the UNK railway ? UNK UNK to UNK spain ? is what\n",
      "> American Sentinel University was established through the joining of two separate schools: American College of Computer & Information Sciences and American Graduate School of Management. \n",
      "Bot: what was the name of the college college college college of the college of american idol ? other areas ?\n",
      "> March 2009, American Sentinel University's undergraduate and graduate nursing programs earned accreditation from the Commission on Collegiate Nursing Education (CCNE).\n",
      "Bot: what is the name of the main american student college of the college ? t receive what ? they receive ? they receive what ? they ? they\n",
      "> In 2016, a report from the Department of Housing and Urban Development (HUD) shows that the U.S. state of Oregon had an estimated homeless population of 13,238 with about 60.5% of these people still unsheltered.\n",
      "Bot: what percentage of the population of the u .s . were recorded in 2016 ?\n",
      "> In the summer of 1109 the Polish ruler Bolesław III organized an expedition into Pomerania in order to secure his northern boundary.\n",
      "Bot: what was the name of the polish dynasty ? spain ? spain was what ?\n",
      "> On 10 August 1109. Bolesław's force, which was besieging Naklo engaged the Pomeranian relief forces and defeated them.\n",
      "Bot: what was the UNK UNK ? t them ? are they are called what ? they them are they called ? they them they were them to do what ? they them t them they were them ? they them they were them to do what ? they them t ? they\n",
      "> Columbus Towers or Torres de Colón is a highrise office building composed of twin towers located at the Plaza de Colón in Madrid, Spain. The building constructed in 1976 was designed by the architect Antonio Lamela.\n",
      "Bot: who designed the building de monica ? ?\n",
      "> Olivier Chapuis (born 10 January 1975) is a French retired competitive ice dancer.\n",
      "Bot: what is the french name for the french ? t receive a UNK ? UNK far .\n",
      "> The Apollo Guidance Computer (AGC) was a digital computer produced for the Apollo program that was installed on board each Apollo command module (CM) and Apollo Lunar Module (LM).\n",
      "Bot: what was the apollo UNK used for ? t launch ? to moon ? t get to the apollo moon ? apollo 2 t launch ? to moon what ? apollo 2 t launch ? t t receive what kind of boundary ? t t t moon ? apollo 2 t to air moon ? t t\n",
      "> The 1998 Chilean telethon (December 4-5, 1988) was the 15th version of the solidarity campaign conducted in Chile. \n",
      "Bot: what was the primary cause of the failure of the nationalization of the ohio ? other states\n",
      "> Wobbuffet is a tall, cyan Pokémon with a soft body. Its eyes usually appear scrunched, and it has a jagged upper lip.\n",
      "Bot: what is the UNK UNK UNK with a body called ? is usually what ?\n",
      "> Wobbuffet is usually a docile Pokémon that will never attack first.\n",
      "Bot: what is the first person who is a person that is not head ? UNK other states ?\n",
      "> However, when it is attacked, it will inflate its body and initiate a counterstrike. When two or more of this Pokémon meet, they will attempt to outlast each other in a battle of endurance.\n",
      "Bot: what is the UNK of a diminutive UNK ? t t other . they would have to be at least a successful ?\n",
      "> However, since neither is able to attack, they may compete to see which can last without food. Because of its overprotective nature regarding its tail and hatred of light, Wobbuffet lives in dark caves.\n",
      "Bot: what kind of nature can a clock give up the environment ? ?\n",
      "> Girafarig is a hoofed quadruped Pokémon with a long neck\n",
      "Bot: what is a long type of flight ? long is a long type . long city ? long . long . long . long . long . long . long . long . long . long . long . long .\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
